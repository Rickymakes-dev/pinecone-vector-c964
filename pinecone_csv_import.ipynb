{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 2023-10-09 T5"
      ],
      "metadata": {
        "id": "eDJrC_knjZBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pinecone-client openai tiktoken langchain unidecode"
      ],
      "metadata": {
        "id": "JZgECPIxA9OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HEe-EQ2VK8d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_path = \"Replace your path here\"\n",
        "pinecone_index = 'Replace your pinecone index here'\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import pinecone\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "# Initialize OpenAI\n",
        "os.environ[\"openai.api_key\"] = getpass.getpass(\"OpenAI API Key:\")\n",
        "\n",
        "# Initialize Pinecone\n",
        "os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\n",
        "os.environ[\"PINECONE_ENV\"] = getpass.getpass(\"Pinecone Environment:\")\n",
        "\n",
        "pinecone.init(\n",
        "\tapi_key=os.getenv(\"PINECONE_API_KEY\"),\n",
        "\tenvironment=os.getenv(\"PINECONE_ENV\")\n",
        ")"
      ],
      "metadata": {
        "id": "ULFBLBZq_upt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to use\n",
        "MODEL = \"text-embedding-ada-002\"\n",
        "\n",
        "# Create a list to hold the embeddings\n",
        "embeddings = []\n",
        "\n",
        "# Batch size for uploading, pinecone recommended around 100 vector / less than 2MB\n",
        "batch_size = 75\n",
        "\n",
        "# Loop through the DataFrame in chunks\n",
        "for i in range(0, len(df), batch_size):\n",
        "    # Get the batch of strings\n",
        "    batch = df.iloc[i:i+batch_size, 1].tolist()\n",
        "\n",
        "    # Create the embeddings\n",
        "    res = openai.Embedding.create(input=batch, engine=MODEL)\n",
        "\n",
        "    # Extract the embeddings and add them to the list\n",
        "    embeddings.extend([record['embedding'] for record in res['data']])"
      ],
      "metadata": {
        "id": "62zPAKD5_wEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "   # Connect to the index\n",
        "index = pinecone.Index(pinecone_index)\n",
        "print(len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZWUV0Cr_yXv",
        "outputId": "fa2c6bb3-5fdb-41ac-c186-ac4fae761e4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a helper function to break the list into chunks\n",
        "def chunks(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "\n",
        "# Import the necessary library\n",
        "from unidecode import unidecode\n",
        "import secrets\n",
        "\n",
        "# Create a list to hold the IDs, embeddings, and metadata\n",
        "ids_embeddings_metadata = []\n",
        "\n",
        "# Loop through the DataFrame\n",
        "for i, row in df.iterrows():\n",
        "\n",
        "    # Generate a 6-digit random hexadecimal number\n",
        "    random_hex = secrets.token_hex(3)\n",
        "\n",
        "    # Convert the ID to ASCII\n",
        "    ascii_id = unidecode(row[0])\n",
        "    # Make sure all the ID are unique, the job title will be parse out by removing the first 7 varchar\n",
        "    unique_id = random_hex + '-' + ascii_id\n",
        "\n",
        "    # Create a dictionary to hold the ID, embedding, and metadata\n",
        "    id_embedding_metadata = {\n",
        "        'id': unique_id,\n",
        "        'values': embeddings[i],\n",
        "        'metadata': {\n",
        "            'row_count': i,\n",
        "            'csv_column_2': row[1],\n",
        "            'csv_column_3': row[2],\n",
        "            'csv_column_4': row[3],\n",
        "            'csv_column_5': row[4],\n",
        "            'file_path': csv_path\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Add the dictionary to the list\n",
        "    ids_embeddings_metadata.append(id_embedding_metadata)\n",
        "\n",
        "# Upsert the vectors with their metadata in chunks\n",
        "for chunk in chunks(ids_embeddings_metadata, batch_size):\n",
        "    index.upsert(vectors=chunk)\n"
      ],
      "metadata": {
        "id": "aMdgBDNbFfUv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}